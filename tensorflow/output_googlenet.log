I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 12
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:06:00.0
Total memory: 12.00GiB
Free memory: 11.87GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 12105628263
I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 12
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 2862 get requests, put_count=2445 evicted_count=1000 eviction_rate=0.408998 and unsatisfied allocation rate=0.530049
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 100 to 110
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 6237 get requests, put_count=6236 evicted_count=1000 eviction_rate=0.160359 and unsatisfied allocation rate=0.164181
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 256 to 281
E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:623] Deallocating stream with pending work
2015-11-11 15:27:40.631786: step 10, duration = 0.054
2015-11-11 15:27:41.171899: step 20, duration = 0.054
2015-11-11 15:27:41.712831: step 30, duration = 0.054
2015-11-11 15:27:42.252970: step 40, duration = 0.054
2015-11-11 15:27:42.794053: step 50, duration = 0.054
2015-11-11 15:27:43.334228: step 60, duration = 0.054
2015-11-11 15:27:43.875158: step 70, duration = 0.054
2015-11-11 15:27:44.415573: step 80, duration = 0.054
2015-11-11 15:27:44.956751: step 90, duration = 0.054
2015-11-11 15:27:45.443141: Forward across 100 steps, 0.054 +/- 0.005 sec / batch
2015-11-11 15:27:58.249311: step 10, duration = 0.596
2015-11-11 15:28:04.211849: step 20, duration = 0.596
2015-11-11 15:28:10.170966: step 30, duration = 0.595
2015-11-11 15:28:16.128262: step 40, duration = 0.595
2015-11-11 15:28:22.088285: step 50, duration = 0.595
2015-11-11 15:28:28.044325: step 60, duration = 0.596
2015-11-11 15:28:34.004426: step 70, duration = 0.596
2015-11-11 15:28:39.957435: step 80, duration = 0.597
2015-11-11 15:28:45.917973: step 90, duration = 0.597
2015-11-11 15:28:51.275416: Forward-backward across 100 steps, 0.590 +/- 0.059 sec / batch
