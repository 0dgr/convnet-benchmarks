I0408 23:27:35.673627 11298 caffe.cpp:212] Use GPU with device ID 0
E0408 23:27:35.893597 11298 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./imagenet_winners/alexnet.prototxt
I0408 23:27:35.893674 11298 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0408 23:27:35.893822 11298 net.cpp:42] Initializing net from parameters: 
name: "alexnet"
input: "data"
input_dim: 128
input_dim: 3
input_dim: 224
input_dim: 224
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1/11x11_s4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv1/relu"
  type: "ReLU"
  bottom: "conv1/11x11_s4"
  top: "conv1/11x11_s4"
}
layer {
  name: "pool1/3x3_s2"
  type: "Pooling"
  bottom: "conv1/11x11_s4"
  top: "pool1/3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2/5x5_s1"
  type: "Convolution"
  bottom: "pool1/3x3_s2"
  top: "conv2/5x5_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 192
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "cpnv2/relu"
  type: "ReLU"
  bottom: "conv2/5x5_s1"
  top: "conv2/5x5_s1"
}
layer {
  name: "pool2/3x3_s2"
  type: "Pooling"
  bottom: "conv2/5x5_s1"
  top: "pool2/3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3/3x3_s1"
  type: "Convolution"
  bottom: "pool2/3x3_s2"
  top: "conv3/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv3/relu"
  type: "ReLU"
  bottom: "conv3/3x3_s1"
  top: "conv3/3x3_s1"
}
layer {
  name: "conv4/3x3_s1"
  type: "Convolution"
  bottom: "conv3/3x3_s1"
  top: "conv4/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv4/relu"
  type: "ReLU"
  bottom: "conv4/3x3_s1"
  top: "conv4/3x3_s1"
}
layer {
  name: "conv5/3x3_s1"
  type: "Convolution"
  bottom: "conv4/3x3_s1"
  top: "conv5/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv5/relu"
  type: "ReLU"
  bottom: "conv5/3x3_s1"
  top: "conv5/3x3_s1"
}
layer {
  name: "pool5/3x3_s2"
  type: "Pooling"
  bottom: "conv5/3x3_s1"
  top: "pool5/3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6-conv"
  type: "Convolution"
  bottom: "pool5/3x3_s2"
  top: "fc6-conv"
  convolution_param {
    num_output: 4096
    kernel_size: 6
  }
}
layer {
  name: "fc7-conv"
  type: "Convolution"
  bottom: "fc6-conv"
  top: "fc7-conv"
  convolution_param {
    num_output: 4096
    kernel_size: 1
  }
}
layer {
  name: "fc8-conv"
  type: "Convolution"
  bottom: "fc7-conv"
  top: "fc8-conv"
  convolution_param {
    num_output: 1000
    kernel_size: 1
  }
}
I0408 23:27:35.893884 11298 net.cpp:340] Input 0 -> data
I0408 23:27:35.893920 11298 layer_factory.hpp:74] Creating layer conv1
I0408 23:27:35.893939 11298 net.cpp:84] Creating Layer conv1
I0408 23:27:35.893944 11298 net.cpp:380] conv1 <- data
I0408 23:27:35.893949 11298 net.cpp:338] conv1 -> conv1/11x11_s4
I0408 23:27:35.893959 11298 net.cpp:113] Setting up conv1
I0408 23:27:35.894467 11298 net.cpp:120] Top shape: 128 64 55 55 (24780800)
I0408 23:27:35.894481 11298 layer_factory.hpp:74] Creating layer conv1/relu
I0408 23:27:35.894487 11298 net.cpp:84] Creating Layer conv1/relu
I0408 23:27:35.894490 11298 net.cpp:380] conv1/relu <- conv1/11x11_s4
I0408 23:27:35.894495 11298 net.cpp:327] conv1/relu -> conv1/11x11_s4 (in-place)
I0408 23:27:35.894500 11298 net.cpp:113] Setting up conv1/relu
I0408 23:27:35.894508 11298 net.cpp:120] Top shape: 128 64 55 55 (24780800)
I0408 23:27:35.894511 11298 layer_factory.hpp:74] Creating layer pool1/3x3_s2
I0408 23:27:35.894516 11298 net.cpp:84] Creating Layer pool1/3x3_s2
I0408 23:27:35.894520 11298 net.cpp:380] pool1/3x3_s2 <- conv1/11x11_s4
I0408 23:27:35.894523 11298 net.cpp:338] pool1/3x3_s2 -> pool1/3x3_s2
I0408 23:27:35.894528 11298 net.cpp:113] Setting up pool1/3x3_s2
I0408 23:27:35.894539 11298 net.cpp:120] Top shape: 128 64 27 27 (5971968)
I0408 23:27:35.894543 11298 layer_factory.hpp:74] Creating layer conv2/5x5_s1
I0408 23:27:35.894548 11298 net.cpp:84] Creating Layer conv2/5x5_s1
I0408 23:27:35.894551 11298 net.cpp:380] conv2/5x5_s1 <- pool1/3x3_s2
I0408 23:27:35.894556 11298 net.cpp:338] conv2/5x5_s1 -> conv2/5x5_s1
I0408 23:27:35.894561 11298 net.cpp:113] Setting up conv2/5x5_s1
I0408 23:27:35.896456 11298 net.cpp:120] Top shape: 128 192 27 27 (17915904)
I0408 23:27:35.896467 11298 layer_factory.hpp:74] Creating layer cpnv2/relu
I0408 23:27:35.896473 11298 net.cpp:84] Creating Layer cpnv2/relu
I0408 23:27:35.896476 11298 net.cpp:380] cpnv2/relu <- conv2/5x5_s1
I0408 23:27:35.896481 11298 net.cpp:327] cpnv2/relu -> conv2/5x5_s1 (in-place)
I0408 23:27:35.896484 11298 net.cpp:113] Setting up cpnv2/relu
I0408 23:27:35.896489 11298 net.cpp:120] Top shape: 128 192 27 27 (17915904)
I0408 23:27:35.896492 11298 layer_factory.hpp:74] Creating layer pool2/3x3_s2
I0408 23:27:35.896497 11298 net.cpp:84] Creating Layer pool2/3x3_s2
I0408 23:27:35.896499 11298 net.cpp:380] pool2/3x3_s2 <- conv2/5x5_s1
I0408 23:27:35.896503 11298 net.cpp:338] pool2/3x3_s2 -> pool2/3x3_s2
I0408 23:27:35.896508 11298 net.cpp:113] Setting up pool2/3x3_s2
I0408 23:27:35.896514 11298 net.cpp:120] Top shape: 128 192 13 13 (4153344)
I0408 23:27:35.896517 11298 layer_factory.hpp:74] Creating layer conv3/3x3_s1
I0408 23:27:35.896522 11298 net.cpp:84] Creating Layer conv3/3x3_s1
I0408 23:27:35.896524 11298 net.cpp:380] conv3/3x3_s1 <- pool2/3x3_s2
I0408 23:27:35.896528 11298 net.cpp:338] conv3/3x3_s1 -> conv3/3x3_s1
I0408 23:27:35.896533 11298 net.cpp:113] Setting up conv3/3x3_s1
I0408 23:27:35.900374 11298 net.cpp:120] Top shape: 128 384 13 13 (8306688)
I0408 23:27:35.900401 11298 layer_factory.hpp:74] Creating layer conv3/relu
I0408 23:27:35.900409 11298 net.cpp:84] Creating Layer conv3/relu
I0408 23:27:35.900413 11298 net.cpp:380] conv3/relu <- conv3/3x3_s1
I0408 23:27:35.900418 11298 net.cpp:327] conv3/relu -> conv3/3x3_s1 (in-place)
I0408 23:27:35.900424 11298 net.cpp:113] Setting up conv3/relu
I0408 23:27:35.900429 11298 net.cpp:120] Top shape: 128 384 13 13 (8306688)
I0408 23:27:35.900431 11298 layer_factory.hpp:74] Creating layer conv4/3x3_s1
I0408 23:27:35.900439 11298 net.cpp:84] Creating Layer conv4/3x3_s1
I0408 23:27:35.900442 11298 net.cpp:380] conv4/3x3_s1 <- conv3/3x3_s1
I0408 23:27:35.900446 11298 net.cpp:338] conv4/3x3_s1 -> conv4/3x3_s1
I0408 23:27:35.900452 11298 net.cpp:113] Setting up conv4/3x3_s1
I0408 23:27:35.905869 11298 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0408 23:27:35.905894 11298 layer_factory.hpp:74] Creating layer conv4/relu
I0408 23:27:35.905901 11298 net.cpp:84] Creating Layer conv4/relu
I0408 23:27:35.905905 11298 net.cpp:380] conv4/relu <- conv4/3x3_s1
I0408 23:27:35.905910 11298 net.cpp:327] conv4/relu -> conv4/3x3_s1 (in-place)
I0408 23:27:35.905925 11298 net.cpp:113] Setting up conv4/relu
I0408 23:27:35.905930 11298 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0408 23:27:35.905932 11298 layer_factory.hpp:74] Creating layer conv5/3x3_s1
I0408 23:27:35.905941 11298 net.cpp:84] Creating Layer conv5/3x3_s1
I0408 23:27:35.905944 11298 net.cpp:380] conv5/3x3_s1 <- conv4/3x3_s1
I0408 23:27:35.905949 11298 net.cpp:338] conv5/3x3_s1 -> conv5/3x3_s1
I0408 23:27:35.905954 11298 net.cpp:113] Setting up conv5/3x3_s1
I0408 23:27:35.909574 11298 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0408 23:27:35.909593 11298 layer_factory.hpp:74] Creating layer conv5/relu
I0408 23:27:35.909600 11298 net.cpp:84] Creating Layer conv5/relu
I0408 23:27:35.909602 11298 net.cpp:380] conv5/relu <- conv5/3x3_s1
I0408 23:27:35.909606 11298 net.cpp:327] conv5/relu -> conv5/3x3_s1 (in-place)
I0408 23:27:35.909611 11298 net.cpp:113] Setting up conv5/relu
I0408 23:27:35.909616 11298 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0408 23:27:35.909620 11298 layer_factory.hpp:74] Creating layer pool5/3x3_s2
I0408 23:27:35.909626 11298 net.cpp:84] Creating Layer pool5/3x3_s2
I0408 23:27:35.909627 11298 net.cpp:380] pool5/3x3_s2 <- conv5/3x3_s1
I0408 23:27:35.909631 11298 net.cpp:338] pool5/3x3_s2 -> pool5/3x3_s2
I0408 23:27:35.909637 11298 net.cpp:113] Setting up pool5/3x3_s2
I0408 23:27:35.909643 11298 net.cpp:120] Top shape: 128 256 6 6 (1179648)
I0408 23:27:35.909647 11298 layer_factory.hpp:74] Creating layer fc6-conv
I0408 23:27:35.909651 11298 net.cpp:84] Creating Layer fc6-conv
I0408 23:27:35.909654 11298 net.cpp:380] fc6-conv <- pool5/3x3_s2
I0408 23:27:35.909659 11298 net.cpp:338] fc6-conv -> fc6-conv
I0408 23:27:35.909664 11298 net.cpp:113] Setting up fc6-conv
I0408 23:27:35.993713 11298 net.cpp:120] Top shape: 128 4096 1 1 (524288)
I0408 23:27:35.993743 11298 layer_factory.hpp:74] Creating layer fc7-conv
I0408 23:27:35.993753 11298 net.cpp:84] Creating Layer fc7-conv
I0408 23:27:35.993757 11298 net.cpp:380] fc7-conv <- fc6-conv
I0408 23:27:35.993763 11298 net.cpp:338] fc7-conv -> fc7-conv
I0408 23:27:35.993772 11298 net.cpp:113] Setting up fc7-conv
I0408 23:27:36.122089 11298 net.cpp:120] Top shape: 128 4096 1 1 (524288)
I0408 23:27:36.122119 11298 layer_factory.hpp:74] Creating layer fc8-conv
I0408 23:27:36.122128 11298 net.cpp:84] Creating Layer fc8-conv
I0408 23:27:36.122133 11298 net.cpp:380] fc8-conv <- fc7-conv
I0408 23:27:36.122139 11298 net.cpp:338] fc8-conv -> fc8-conv
I0408 23:27:36.122146 11298 net.cpp:113] Setting up fc8-conv
I0408 23:27:36.147285 11298 net.cpp:120] Top shape: 128 1000 1 1 (128000)
I0408 23:27:36.147313 11298 net.cpp:169] fc8-conv does not need backward computation.
I0408 23:27:36.147316 11298 net.cpp:169] fc7-conv does not need backward computation.
I0408 23:27:36.147320 11298 net.cpp:169] fc6-conv does not need backward computation.
I0408 23:27:36.147322 11298 net.cpp:169] pool5/3x3_s2 does not need backward computation.
I0408 23:27:36.147325 11298 net.cpp:169] conv5/relu does not need backward computation.
I0408 23:27:36.147327 11298 net.cpp:169] conv5/3x3_s1 does not need backward computation.
I0408 23:27:36.147330 11298 net.cpp:169] conv4/relu does not need backward computation.
I0408 23:27:36.147332 11298 net.cpp:169] conv4/3x3_s1 does not need backward computation.
I0408 23:27:36.147336 11298 net.cpp:169] conv3/relu does not need backward computation.
I0408 23:27:36.147337 11298 net.cpp:169] conv3/3x3_s1 does not need backward computation.
I0408 23:27:36.147341 11298 net.cpp:169] pool2/3x3_s2 does not need backward computation.
I0408 23:27:36.147342 11298 net.cpp:169] cpnv2/relu does not need backward computation.
I0408 23:27:36.147346 11298 net.cpp:169] conv2/5x5_s1 does not need backward computation.
I0408 23:27:36.147347 11298 net.cpp:169] pool1/3x3_s2 does not need backward computation.
I0408 23:27:36.147351 11298 net.cpp:169] conv1/relu does not need backward computation.
I0408 23:27:36.147353 11298 net.cpp:169] conv1 does not need backward computation.
I0408 23:27:36.147357 11298 net.cpp:205] This network produces output fc8-conv
I0408 23:27:36.147384 11298 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0408 23:27:36.147392 11298 net.cpp:217] Network initialization done.
I0408 23:27:36.147394 11298 net.cpp:218] Memory required for data: 546557952
I0408 23:27:36.147457 11298 caffe.cpp:224] Performing Forward
I0408 23:27:36.601531 11298 caffe.cpp:229] Initial loss: 0
I0408 23:27:36.601572 11298 caffe.cpp:230] Performing Backward
I0408 23:27:37.340370 11298 caffe.cpp:238] *** Benchmark begins ***
I0408 23:27:37.340391 11298 caffe.cpp:239] Testing for 10 iterations.
I0408 23:27:38.492202 11298 caffe.cpp:270] Iteration: 1 forward-backward time: 1115.12 ms.
I0408 23:27:40.033017 11298 caffe.cpp:270] Iteration: 2 forward-backward time: 1540.75 ms.
I0408 23:27:41.968022 11298 caffe.cpp:270] Iteration: 3 forward-backward time: 1933.79 ms.
I0408 23:27:43.948986 11298 caffe.cpp:270] Iteration: 4 forward-backward time: 1979.48 ms.
I0408 23:27:45.863242 11298 caffe.cpp:270] Iteration: 5 forward-backward time: 1914.18 ms.
I0408 23:27:47.652629 11298 caffe.cpp:270] Iteration: 6 forward-backward time: 1789.32 ms.
I0408 23:27:49.495738 11298 caffe.cpp:270] Iteration: 7 forward-backward time: 1838.98 ms.
I0408 23:27:51.448470 11298 caffe.cpp:270] Iteration: 8 forward-backward time: 1952.66 ms.
I0408 23:27:53.422359 11298 caffe.cpp:270] Iteration: 9 forward-backward time: 1972.71 ms.
I0408 23:27:55.415953 11298 caffe.cpp:270] Iteration: 10 forward-backward time: 1991.03 ms.
I0408 23:27:55.415990 11298 caffe.cpp:273] Average time per layer: 
I0408 23:27:55.415994 11298 caffe.cpp:276]      conv1	forward: 23.2096 ms.
I0408 23:27:55.415999 11298 caffe.cpp:279]      conv1	backward: 75.1043 ms.
I0408 23:27:55.416002 11298 caffe.cpp:276] conv1/relu	forward: 0.792896 ms.
I0408 23:27:55.416007 11298 caffe.cpp:279] conv1/relu	backward: 1.71397 ms.
I0408 23:27:55.416009 11298 caffe.cpp:276] pool1/3x3_s2	forward: 0.900413 ms.
I0408 23:27:55.416013 11298 caffe.cpp:279] pool1/3x3_s2	backward: 4.85603 ms.
I0408 23:27:55.416016 11298 caffe.cpp:276] conv2/5x5_s1	forward: 45.3922 ms.
I0408 23:27:55.416019 11298 caffe.cpp:279] conv2/5x5_s1	backward: 82.4727 ms.
I0408 23:27:55.416023 11298 caffe.cpp:276] cpnv2/relu	forward: 0.575181 ms.
I0408 23:27:55.416026 11298 caffe.cpp:279] cpnv2/relu	backward: 0.848291 ms.
I0408 23:27:55.416029 11298 caffe.cpp:276] pool2/3x3_s2	forward: 0.659162 ms.
I0408 23:27:55.416033 11298 caffe.cpp:279] pool2/3x3_s2	backward: 4.28801 ms.
I0408 23:27:55.416036 11298 caffe.cpp:276] conv3/3x3_s1	forward: 40.5894 ms.
I0408 23:27:55.416039 11298 caffe.cpp:279] conv3/3x3_s1	backward: 48.7334 ms.
I0408 23:27:55.416043 11298 caffe.cpp:276] conv3/relu	forward: 0.269318 ms.
I0408 23:27:55.416046 11298 caffe.cpp:279] conv3/relu	backward: 0.397622 ms.
I0408 23:27:55.416049 11298 caffe.cpp:276] conv4/3x3_s1	forward: 43.2736 ms.
I0408 23:27:55.416052 11298 caffe.cpp:279] conv4/3x3_s1	backward: 77.1831 ms.
I0408 23:27:55.416055 11298 caffe.cpp:276] conv4/relu	forward: 0.179814 ms.
I0408 23:27:55.416059 11298 caffe.cpp:279] conv4/relu	backward: 0.267574 ms.
I0408 23:27:55.416061 11298 caffe.cpp:276] conv5/3x3_s1	forward: 35.5345 ms.
I0408 23:27:55.416065 11298 caffe.cpp:279] conv5/3x3_s1	backward: 45.324 ms.
I0408 23:27:55.416069 11298 caffe.cpp:276] conv5/relu	forward: 0.180429 ms.
I0408 23:27:55.416072 11298 caffe.cpp:279] conv5/relu	backward: 0.270243 ms.
I0408 23:27:55.416075 11298 caffe.cpp:276] pool5/3x3_s2	forward: 0.223568 ms.
I0408 23:27:55.416079 11298 caffe.cpp:279] pool5/3x3_s2	backward: 0.837018 ms.
I0408 23:27:55.416081 11298 caffe.cpp:276]   fc6-conv	forward: 281.054 ms.
I0408 23:27:55.416085 11298 caffe.cpp:279]   fc6-conv	backward: 519.943 ms.
I0408 23:27:55.416088 11298 caffe.cpp:276]   fc7-conv	forward: 116.102 ms.
I0408 23:27:55.416091 11298 caffe.cpp:279]   fc7-conv	backward: 227.373 ms.
I0408 23:27:55.416095 11298 caffe.cpp:276]   fc8-conv	forward: 34.4808 ms.
I0408 23:27:55.416098 11298 caffe.cpp:279]   fc8-conv	backward: 63.8455 ms.
I0408 23:27:55.416131 11298 caffe.cpp:284] Average Forward pass: 635.713 ms.
I0408 23:27:55.416136 11298 caffe.cpp:286] Average Backward pass: 1165.64 ms.
I0408 23:27:55.416149 11298 caffe.cpp:288] Average Forward-Backward: 1803.9 ms.
I0408 23:27:55.416154 11298 caffe.cpp:290] Total Time: 18039 ms.
I0408 23:27:55.416159 11298 caffe.cpp:291] *** Benchmark ends ***
I0413 19:14:01.588980 11742 caffe.cpp:212] Use GPU with device ID 0
E0413 19:14:01.809979 11742 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./imagenet_winners/alexnet.prototxt
I0413 19:14:01.810050 11742 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0413 19:14:01.810191 11742 net.cpp:42] Initializing net from parameters: 
name: "alexnet"
input: "data"
input_dim: 128
input_dim: 3
input_dim: 224
input_dim: 224
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1/11x11_s4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv1/relu"
  type: "ReLU"
  bottom: "conv1/11x11_s4"
  top: "conv1/11x11_s4"
}
layer {
  name: "pool1/3x3_s2"
  type: "Pooling"
  bottom: "conv1/11x11_s4"
  top: "pool1/3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2/5x5_s1"
  type: "Convolution"
  bottom: "pool1/3x3_s2"
  top: "conv2/5x5_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 192
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "cpnv2/relu"
  type: "ReLU"
  bottom: "conv2/5x5_s1"
  top: "conv2/5x5_s1"
}
layer {
  name: "pool2/3x3_s2"
  type: "Pooling"
  bottom: "conv2/5x5_s1"
  top: "pool2/3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3/3x3_s1"
  type: "Convolution"
  bottom: "pool2/3x3_s2"
  top: "conv3/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv3/relu"
  type: "ReLU"
  bottom: "conv3/3x3_s1"
  top: "conv3/3x3_s1"
}
layer {
  name: "conv4/3x3_s1"
  type: "Convolution"
  bottom: "conv3/3x3_s1"
  top: "conv4/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv4/relu"
  type: "ReLU"
  bottom: "conv4/3x3_s1"
  top: "conv4/3x3_s1"
}
layer {
  name: "conv5/3x3_s1"
  type: "Convolution"
  bottom: "conv4/3x3_s1"
  top: "conv5/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv5/relu"
  type: "ReLU"
  bottom: "conv5/3x3_s1"
  top: "conv5/3x3_s1"
}
layer {
  name: "pool5/3x3_s2"
  type: "Pooling"
  bottom: "conv5/3x3_s1"
  top: "pool5/3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6-conv"
  type: "Convolution"
  bottom: "pool5/3x3_s2"
  top: "fc6-conv"
  convolution_param {
    num_output: 4096
    kernel_size: 6
  }
}
layer {
  name: "fc7-conv"
  type: "Convolution"
  bottom: "fc6-conv"
  top: "fc7-conv"
  convolution_param {
    num_output: 4096
    kernel_size: 1
  }
}
layer {
  name: "fc8-conv"
  type: "Convolution"
  bottom: "fc7-conv"
  top: "fc8-conv"
  convolution_param {
    num_output: 1000
    kernel_size: 1
  }
}
I0413 19:14:01.810252 11742 net.cpp:340] Input 0 -> data
I0413 19:14:01.810284 11742 layer_factory.hpp:74] Creating layer conv1
I0413 19:14:01.810299 11742 net.cpp:84] Creating Layer conv1
I0413 19:14:01.810304 11742 net.cpp:380] conv1 <- data
I0413 19:14:01.810310 11742 net.cpp:338] conv1 -> conv1/11x11_s4
I0413 19:14:01.810319 11742 net.cpp:113] Setting up conv1
I0413 19:14:01.810820 11742 net.cpp:120] Top shape: 128 64 55 55 (24780800)
I0413 19:14:01.810833 11742 layer_factory.hpp:74] Creating layer conv1/relu
I0413 19:14:01.810839 11742 net.cpp:84] Creating Layer conv1/relu
I0413 19:14:01.810842 11742 net.cpp:380] conv1/relu <- conv1/11x11_s4
I0413 19:14:01.810847 11742 net.cpp:327] conv1/relu -> conv1/11x11_s4 (in-place)
I0413 19:14:01.810852 11742 net.cpp:113] Setting up conv1/relu
I0413 19:14:01.810859 11742 net.cpp:120] Top shape: 128 64 55 55 (24780800)
I0413 19:14:01.810863 11742 layer_factory.hpp:74] Creating layer pool1/3x3_s2
I0413 19:14:01.810868 11742 net.cpp:84] Creating Layer pool1/3x3_s2
I0413 19:14:01.810870 11742 net.cpp:380] pool1/3x3_s2 <- conv1/11x11_s4
I0413 19:14:01.810873 11742 net.cpp:338] pool1/3x3_s2 -> pool1/3x3_s2
I0413 19:14:01.810878 11742 net.cpp:113] Setting up pool1/3x3_s2
I0413 19:14:01.810889 11742 net.cpp:120] Top shape: 128 64 27 27 (5971968)
I0413 19:14:01.810892 11742 layer_factory.hpp:74] Creating layer conv2/5x5_s1
I0413 19:14:01.810897 11742 net.cpp:84] Creating Layer conv2/5x5_s1
I0413 19:14:01.810900 11742 net.cpp:380] conv2/5x5_s1 <- pool1/3x3_s2
I0413 19:14:01.810904 11742 net.cpp:338] conv2/5x5_s1 -> conv2/5x5_s1
I0413 19:14:01.810909 11742 net.cpp:113] Setting up conv2/5x5_s1
I0413 19:14:01.812795 11742 net.cpp:120] Top shape: 128 192 27 27 (17915904)
I0413 19:14:01.812806 11742 layer_factory.hpp:74] Creating layer cpnv2/relu
I0413 19:14:01.812811 11742 net.cpp:84] Creating Layer cpnv2/relu
I0413 19:14:01.812814 11742 net.cpp:380] cpnv2/relu <- conv2/5x5_s1
I0413 19:14:01.812818 11742 net.cpp:327] cpnv2/relu -> conv2/5x5_s1 (in-place)
I0413 19:14:01.812822 11742 net.cpp:113] Setting up cpnv2/relu
I0413 19:14:01.812826 11742 net.cpp:120] Top shape: 128 192 27 27 (17915904)
I0413 19:14:01.812829 11742 layer_factory.hpp:74] Creating layer pool2/3x3_s2
I0413 19:14:01.812832 11742 net.cpp:84] Creating Layer pool2/3x3_s2
I0413 19:14:01.812835 11742 net.cpp:380] pool2/3x3_s2 <- conv2/5x5_s1
I0413 19:14:01.812839 11742 net.cpp:338] pool2/3x3_s2 -> pool2/3x3_s2
I0413 19:14:01.812844 11742 net.cpp:113] Setting up pool2/3x3_s2
I0413 19:14:01.812849 11742 net.cpp:120] Top shape: 128 192 13 13 (4153344)
I0413 19:14:01.812852 11742 layer_factory.hpp:74] Creating layer conv3/3x3_s1
I0413 19:14:01.812857 11742 net.cpp:84] Creating Layer conv3/3x3_s1
I0413 19:14:01.812860 11742 net.cpp:380] conv3/3x3_s1 <- pool2/3x3_s2
I0413 19:14:01.812865 11742 net.cpp:338] conv3/3x3_s1 -> conv3/3x3_s1
I0413 19:14:01.812870 11742 net.cpp:113] Setting up conv3/3x3_s1
I0413 19:14:01.816896 11742 net.cpp:120] Top shape: 128 384 13 13 (8306688)
I0413 19:14:01.816906 11742 layer_factory.hpp:74] Creating layer conv3/relu
I0413 19:14:01.816911 11742 net.cpp:84] Creating Layer conv3/relu
I0413 19:14:01.816915 11742 net.cpp:380] conv3/relu <- conv3/3x3_s1
I0413 19:14:01.816917 11742 net.cpp:327] conv3/relu -> conv3/3x3_s1 (in-place)
I0413 19:14:01.816922 11742 net.cpp:113] Setting up conv3/relu
I0413 19:14:01.816926 11742 net.cpp:120] Top shape: 128 384 13 13 (8306688)
I0413 19:14:01.816928 11742 layer_factory.hpp:74] Creating layer conv4/3x3_s1
I0413 19:14:01.816933 11742 net.cpp:84] Creating Layer conv4/3x3_s1
I0413 19:14:01.816936 11742 net.cpp:380] conv4/3x3_s1 <- conv3/3x3_s1
I0413 19:14:01.816941 11742 net.cpp:338] conv4/3x3_s1 -> conv4/3x3_s1
I0413 19:14:01.816946 11742 net.cpp:113] Setting up conv4/3x3_s1
I0413 19:14:01.821977 11742 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0413 19:14:01.821990 11742 layer_factory.hpp:74] Creating layer conv4/relu
I0413 19:14:01.821997 11742 net.cpp:84] Creating Layer conv4/relu
I0413 19:14:01.822000 11742 net.cpp:380] conv4/relu <- conv4/3x3_s1
I0413 19:14:01.822005 11742 net.cpp:327] conv4/relu -> conv4/3x3_s1 (in-place)
I0413 19:14:01.822008 11742 net.cpp:113] Setting up conv4/relu
I0413 19:14:01.822012 11742 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0413 19:14:01.822016 11742 layer_factory.hpp:74] Creating layer conv5/3x3_s1
I0413 19:14:01.822021 11742 net.cpp:84] Creating Layer conv5/3x3_s1
I0413 19:14:01.822023 11742 net.cpp:380] conv5/3x3_s1 <- conv4/3x3_s1
I0413 19:14:01.822028 11742 net.cpp:338] conv5/3x3_s1 -> conv5/3x3_s1
I0413 19:14:01.822032 11742 net.cpp:113] Setting up conv5/3x3_s1
I0413 19:14:01.825611 11742 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0413 19:14:01.825623 11742 layer_factory.hpp:74] Creating layer conv5/relu
I0413 19:14:01.825626 11742 net.cpp:84] Creating Layer conv5/relu
I0413 19:14:01.825629 11742 net.cpp:380] conv5/relu <- conv5/3x3_s1
I0413 19:14:01.825634 11742 net.cpp:327] conv5/relu -> conv5/3x3_s1 (in-place)
I0413 19:14:01.825637 11742 net.cpp:113] Setting up conv5/relu
I0413 19:14:01.825641 11742 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0413 19:14:01.825644 11742 layer_factory.hpp:74] Creating layer pool5/3x3_s2
I0413 19:14:01.825649 11742 net.cpp:84] Creating Layer pool5/3x3_s2
I0413 19:14:01.825651 11742 net.cpp:380] pool5/3x3_s2 <- conv5/3x3_s1
I0413 19:14:01.825656 11742 net.cpp:338] pool5/3x3_s2 -> pool5/3x3_s2
I0413 19:14:01.825661 11742 net.cpp:113] Setting up pool5/3x3_s2
I0413 19:14:01.825667 11742 net.cpp:120] Top shape: 128 256 6 6 (1179648)
I0413 19:14:01.825670 11742 layer_factory.hpp:74] Creating layer fc6-conv
I0413 19:14:01.825676 11742 net.cpp:84] Creating Layer fc6-conv
I0413 19:14:01.825680 11742 net.cpp:380] fc6-conv <- pool5/3x3_s2
I0413 19:14:01.825683 11742 net.cpp:338] fc6-conv -> fc6-conv
I0413 19:14:01.825687 11742 net.cpp:113] Setting up fc6-conv
I0413 19:14:01.856766 11742 net.cpp:120] Top shape: 128 4096 1 1 (524288)
I0413 19:14:01.856794 11742 layer_factory.hpp:74] Creating layer fc7-conv
I0413 19:14:01.856804 11742 net.cpp:84] Creating Layer fc7-conv
I0413 19:14:01.856808 11742 net.cpp:380] fc7-conv <- fc6-conv
I0413 19:14:01.856814 11742 net.cpp:338] fc7-conv -> fc7-conv
I0413 19:14:01.856822 11742 net.cpp:113] Setting up fc7-conv
I0413 19:14:01.870949 11742 net.cpp:120] Top shape: 128 4096 1 1 (524288)
I0413 19:14:01.870977 11742 layer_factory.hpp:74] Creating layer fc8-conv
I0413 19:14:01.870986 11742 net.cpp:84] Creating Layer fc8-conv
I0413 19:14:01.870990 11742 net.cpp:380] fc8-conv <- fc7-conv
I0413 19:14:01.870997 11742 net.cpp:338] fc8-conv -> fc8-conv
I0413 19:14:01.871004 11742 net.cpp:113] Setting up fc8-conv
I0413 19:14:01.875416 11742 net.cpp:120] Top shape: 128 1000 1 1 (128000)
I0413 19:14:01.875442 11742 net.cpp:169] fc8-conv does not need backward computation.
I0413 19:14:01.875447 11742 net.cpp:169] fc7-conv does not need backward computation.
I0413 19:14:01.875449 11742 net.cpp:169] fc6-conv does not need backward computation.
I0413 19:14:01.875452 11742 net.cpp:169] pool5/3x3_s2 does not need backward computation.
I0413 19:14:01.875454 11742 net.cpp:169] conv5/relu does not need backward computation.
I0413 19:14:01.875458 11742 net.cpp:169] conv5/3x3_s1 does not need backward computation.
I0413 19:14:01.875460 11742 net.cpp:169] conv4/relu does not need backward computation.
I0413 19:14:01.875463 11742 net.cpp:169] conv4/3x3_s1 does not need backward computation.
I0413 19:14:01.875465 11742 net.cpp:169] conv3/relu does not need backward computation.
I0413 19:14:01.875468 11742 net.cpp:169] conv3/3x3_s1 does not need backward computation.
I0413 19:14:01.875470 11742 net.cpp:169] pool2/3x3_s2 does not need backward computation.
I0413 19:14:01.875473 11742 net.cpp:169] cpnv2/relu does not need backward computation.
I0413 19:14:01.875475 11742 net.cpp:169] conv2/5x5_s1 does not need backward computation.
I0413 19:14:01.875478 11742 net.cpp:169] pool1/3x3_s2 does not need backward computation.
I0413 19:14:01.875489 11742 net.cpp:169] conv1/relu does not need backward computation.
I0413 19:14:01.875493 11742 net.cpp:169] conv1 does not need backward computation.
I0413 19:14:01.875496 11742 net.cpp:205] This network produces output fc8-conv
I0413 19:14:01.875511 11742 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0413 19:14:01.875519 11742 net.cpp:217] Network initialization done.
I0413 19:14:01.875520 11742 net.cpp:218] Memory required for data: 546557952
I0413 19:14:01.875583 11742 caffe.cpp:224] Performing Forward
I0413 19:14:02.308010 11742 caffe.cpp:229] Initial loss: 0
I0413 19:14:02.308048 11742 caffe.cpp:230] Performing Backward
I0413 19:14:03.001006 11742 caffe.cpp:238] *** Benchmark begins ***
I0413 19:14:03.001019 11742 caffe.cpp:239] Testing for 10 iterations.
I0413 19:14:04.131523 11742 caffe.cpp:270] Iteration: 1 forward-backward time: 1095.02 ms.
I0413 19:14:05.226881 11742 caffe.cpp:270] Iteration: 2 forward-backward time: 1095.3 ms.
I0413 19:14:06.322649 11742 caffe.cpp:270] Iteration: 3 forward-backward time: 1095.72 ms.
I0413 19:14:07.417791 11742 caffe.cpp:270] Iteration: 4 forward-backward time: 1095.09 ms.
I0413 19:14:08.512827 11742 caffe.cpp:270] Iteration: 5 forward-backward time: 1094.99 ms.
I0413 19:14:09.608193 11742 caffe.cpp:270] Iteration: 6 forward-backward time: 1095.32 ms.
I0413 19:14:10.703835 11742 caffe.cpp:270] Iteration: 7 forward-backward time: 1095.59 ms.
I0413 19:14:11.799108 11742 caffe.cpp:270] Iteration: 8 forward-backward time: 1095.22 ms.
I0413 19:14:12.894769 11742 caffe.cpp:270] Iteration: 9 forward-backward time: 1095.61 ms.
I0413 19:14:13.990783 11742 caffe.cpp:270] Iteration: 10 forward-backward time: 1095.96 ms.
I0413 19:14:13.990799 11742 caffe.cpp:273] Average time per layer: 
I0413 19:14:13.990803 11742 caffe.cpp:276]      conv1	forward: 16.1055 ms.
I0413 19:14:13.990808 11742 caffe.cpp:279]      conv1	backward: 41.9029 ms.
I0413 19:14:13.990811 11742 caffe.cpp:276] conv1/relu	forward: 0.785414 ms.
I0413 19:14:13.990814 11742 caffe.cpp:279] conv1/relu	backward: 1.1691 ms.
I0413 19:14:13.990818 11742 caffe.cpp:276] pool1/3x3_s2	forward: 0.882413 ms.
I0413 19:14:13.990821 11742 caffe.cpp:279] pool1/3x3_s2	backward: 3.53434 ms.
I0413 19:14:13.990825 11742 caffe.cpp:276] conv2/5x5_s1	forward: 30.313 ms.
I0413 19:14:13.990828 11742 caffe.cpp:279] conv2/5x5_s1	backward: 47.3891 ms.
I0413 19:14:13.990833 11742 caffe.cpp:276] cpnv2/relu	forward: 0.569136 ms.
I0413 19:14:13.990835 11742 caffe.cpp:279] cpnv2/relu	backward: 0.845933 ms.
I0413 19:14:13.990839 11742 caffe.cpp:276] pool2/3x3_s2	forward: 0.654141 ms.
I0413 19:14:13.990841 11742 caffe.cpp:279] pool2/3x3_s2	backward: 2.60434 ms.
I0413 19:14:13.990845 11742 caffe.cpp:276] conv3/3x3_s1	forward: 22.4254 ms.
I0413 19:14:13.990849 11742 caffe.cpp:279] conv3/3x3_s1	backward: 32.6761 ms.
I0413 19:14:13.990852 11742 caffe.cpp:276] conv3/relu	forward: 0.265318 ms.
I0413 19:14:13.990855 11742 caffe.cpp:279] conv3/relu	backward: 0.39497 ms.
I0413 19:14:13.990859 11742 caffe.cpp:276] conv4/3x3_s1	forward: 28.1351 ms.
I0413 19:14:13.990862 11742 caffe.cpp:279] conv4/3x3_s1	backward: 40.8515 ms.
I0413 19:14:13.990865 11742 caffe.cpp:276] conv4/relu	forward: 0.17879 ms.
I0413 19:14:13.990869 11742 caffe.cpp:279] conv4/relu	backward: 0.26695 ms.
I0413 19:14:13.990872 11742 caffe.cpp:276] conv5/3x3_s1	forward: 21.0041 ms.
I0413 19:14:13.990875 11742 caffe.cpp:279] conv5/3x3_s1	backward: 28.9541 ms.
I0413 19:14:13.990878 11742 caffe.cpp:276] conv5/relu	forward: 0.177779 ms.
I0413 19:14:13.990882 11742 caffe.cpp:279] conv5/relu	backward: 0.266259 ms.
I0413 19:14:13.990885 11742 caffe.cpp:276] pool5/3x3_s2	forward: 0.222323 ms.
I0413 19:14:13.990888 11742 caffe.cpp:279] pool5/3x3_s2	backward: 0.814288 ms.
I0413 19:14:13.990891 11742 caffe.cpp:276]   fc6-conv	forward: 177.845 ms.
I0413 19:14:13.990895 11742 caffe.cpp:279]   fc6-conv	backward: 317.396 ms.
I0413 19:14:13.990898 11742 caffe.cpp:276]   fc7-conv	forward: 73.0515 ms.
I0413 19:14:13.990902 11742 caffe.cpp:279]   fc7-conv	backward: 143.55 ms.
I0413 19:14:13.990911 11742 caffe.cpp:276]   fc8-conv	forward: 22.5497 ms.
I0413 19:14:13.990916 11742 caffe.cpp:279]   fc8-conv	backward: 37.3428 ms.
I0413 19:14:13.990927 11742 caffe.cpp:284] Average Forward pass: 395.284 ms.
I0413 19:14:13.990932 11742 caffe.cpp:286] Average Backward pass: 700.083 ms.
I0413 19:14:13.990937 11742 caffe.cpp:288] Average Forward-Backward: 1095.42 ms.
I0413 19:14:13.990942 11742 caffe.cpp:290] Total Time: 10954.2 ms.
I0413 19:14:13.990947 11742 caffe.cpp:291] *** Benchmark ends ***
I0415 15:47:03.991626 32479 caffe.cpp:212] Use GPU with device ID 0
E0415 15:47:04.209281 32479 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./imagenet_winners/alexnet.prototxt
I0415 15:47:04.209350 32479 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0415 15:47:04.209492 32479 net.cpp:42] Initializing net from parameters: 
name: "alexnet"
input: "data"
input_dim: 128
input_dim: 3
input_dim: 224
input_dim: 224
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1/11x11_s4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv1/relu"
  type: "ReLU"
  bottom: "conv1/11x11_s4"
  top: "conv1/11x11_s4"
}
layer {
  name: "pool1/3x3_s2"
  type: "Pooling"
  bottom: "conv1/11x11_s4"
  top: "pool1/3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2/5x5_s1"
  type: "Convolution"
  bottom: "pool1/3x3_s2"
  top: "conv2/5x5_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 192
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "cpnv2/relu"
  type: "ReLU"
  bottom: "conv2/5x5_s1"
  top: "conv2/5x5_s1"
}
layer {
  name: "pool2/3x3_s2"
  type: "Pooling"
  bottom: "conv2/5x5_s1"
  top: "pool2/3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3/3x3_s1"
  type: "Convolution"
  bottom: "pool2/3x3_s2"
  top: "conv3/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv3/relu"
  type: "ReLU"
  bottom: "conv3/3x3_s1"
  top: "conv3/3x3_s1"
}
layer {
  name: "conv4/3x3_s1"
  type: "Convolution"
  bottom: "conv3/3x3_s1"
  top: "conv4/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv4/relu"
  type: "ReLU"
  bottom: "conv4/3x3_s1"
  top: "conv4/3x3_s1"
}
layer {
  name: "conv5/3x3_s1"
  type: "Convolution"
  bottom: "conv4/3x3_s1"
  top: "conv5/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv5/relu"
  type: "ReLU"
  bottom: "conv5/3x3_s1"
  top: "conv5/3x3_s1"
}
layer {
  name: "pool5/3x3_s2"
  type: "Pooling"
  bottom: "conv5/3x3_s1"
  top: "pool5/3x3_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6-conv"
  type: "Convolution"
  bottom: "pool5/3x3_s2"
  top: "fc6-conv"
  convolution_param {
    num_output: 4096
    kernel_size: 6
  }
}
layer {
  name: "fc7-conv"
  type: "Convolution"
  bottom: "fc6-conv"
  top: "fc7-conv"
  convolution_param {
    num_output: 4096
    kernel_size: 1
  }
}
layer {
  name: "fc8-conv"
  type: "Convolution"
  bottom: "fc7-conv"
  top: "fc8-conv"
  convolution_param {
    num_output: 1000
    kernel_size: 1
  }
}
I0415 15:47:04.209556 32479 net.cpp:340] Input 0 -> data
I0415 15:47:04.209584 32479 layer_factory.hpp:74] Creating layer conv1
I0415 15:47:04.209600 32479 net.cpp:84] Creating Layer conv1
I0415 15:47:04.209605 32479 net.cpp:380] conv1 <- data
I0415 15:47:04.209611 32479 net.cpp:338] conv1 -> conv1/11x11_s4
I0415 15:47:04.209619 32479 net.cpp:113] Setting up conv1
I0415 15:47:04.210111 32479 net.cpp:120] Top shape: 128 64 55 55 (24780800)
I0415 15:47:04.210125 32479 layer_factory.hpp:74] Creating layer conv1/relu
I0415 15:47:04.210131 32479 net.cpp:84] Creating Layer conv1/relu
I0415 15:47:04.210134 32479 net.cpp:380] conv1/relu <- conv1/11x11_s4
I0415 15:47:04.210139 32479 net.cpp:327] conv1/relu -> conv1/11x11_s4 (in-place)
I0415 15:47:04.210144 32479 net.cpp:113] Setting up conv1/relu
I0415 15:47:04.210152 32479 net.cpp:120] Top shape: 128 64 55 55 (24780800)
I0415 15:47:04.210155 32479 layer_factory.hpp:74] Creating layer pool1/3x3_s2
I0415 15:47:04.210160 32479 net.cpp:84] Creating Layer pool1/3x3_s2
I0415 15:47:04.210162 32479 net.cpp:380] pool1/3x3_s2 <- conv1/11x11_s4
I0415 15:47:04.210166 32479 net.cpp:338] pool1/3x3_s2 -> pool1/3x3_s2
I0415 15:47:04.210171 32479 net.cpp:113] Setting up pool1/3x3_s2
I0415 15:47:04.210182 32479 net.cpp:120] Top shape: 128 64 27 27 (5971968)
I0415 15:47:04.210186 32479 layer_factory.hpp:74] Creating layer conv2/5x5_s1
I0415 15:47:04.210191 32479 net.cpp:84] Creating Layer conv2/5x5_s1
I0415 15:47:04.210193 32479 net.cpp:380] conv2/5x5_s1 <- pool1/3x3_s2
I0415 15:47:04.210197 32479 net.cpp:338] conv2/5x5_s1 -> conv2/5x5_s1
I0415 15:47:04.210202 32479 net.cpp:113] Setting up conv2/5x5_s1
I0415 15:47:04.212039 32479 net.cpp:120] Top shape: 128 192 27 27 (17915904)
I0415 15:47:04.212050 32479 layer_factory.hpp:74] Creating layer cpnv2/relu
I0415 15:47:04.212055 32479 net.cpp:84] Creating Layer cpnv2/relu
I0415 15:47:04.212059 32479 net.cpp:380] cpnv2/relu <- conv2/5x5_s1
I0415 15:47:04.212062 32479 net.cpp:327] cpnv2/relu -> conv2/5x5_s1 (in-place)
I0415 15:47:04.212066 32479 net.cpp:113] Setting up cpnv2/relu
I0415 15:47:04.212070 32479 net.cpp:120] Top shape: 128 192 27 27 (17915904)
I0415 15:47:04.212074 32479 layer_factory.hpp:74] Creating layer pool2/3x3_s2
I0415 15:47:04.212077 32479 net.cpp:84] Creating Layer pool2/3x3_s2
I0415 15:47:04.212080 32479 net.cpp:380] pool2/3x3_s2 <- conv2/5x5_s1
I0415 15:47:04.212083 32479 net.cpp:338] pool2/3x3_s2 -> pool2/3x3_s2
I0415 15:47:04.212088 32479 net.cpp:113] Setting up pool2/3x3_s2
I0415 15:47:04.212095 32479 net.cpp:120] Top shape: 128 192 13 13 (4153344)
I0415 15:47:04.212097 32479 layer_factory.hpp:74] Creating layer conv3/3x3_s1
I0415 15:47:04.212101 32479 net.cpp:84] Creating Layer conv3/3x3_s1
I0415 15:47:04.212105 32479 net.cpp:380] conv3/3x3_s1 <- pool2/3x3_s2
I0415 15:47:04.212108 32479 net.cpp:338] conv3/3x3_s1 -> conv3/3x3_s1
I0415 15:47:04.212113 32479 net.cpp:113] Setting up conv3/3x3_s1
I0415 15:47:04.216073 32479 net.cpp:120] Top shape: 128 384 13 13 (8306688)
I0415 15:47:04.216083 32479 layer_factory.hpp:74] Creating layer conv3/relu
I0415 15:47:04.216087 32479 net.cpp:84] Creating Layer conv3/relu
I0415 15:47:04.216090 32479 net.cpp:380] conv3/relu <- conv3/3x3_s1
I0415 15:47:04.216094 32479 net.cpp:327] conv3/relu -> conv3/3x3_s1 (in-place)
I0415 15:47:04.216099 32479 net.cpp:113] Setting up conv3/relu
I0415 15:47:04.216102 32479 net.cpp:120] Top shape: 128 384 13 13 (8306688)
I0415 15:47:04.216105 32479 layer_factory.hpp:74] Creating layer conv4/3x3_s1
I0415 15:47:04.216110 32479 net.cpp:84] Creating Layer conv4/3x3_s1
I0415 15:47:04.216116 32479 net.cpp:380] conv4/3x3_s1 <- conv3/3x3_s1
I0415 15:47:04.216121 32479 net.cpp:338] conv4/3x3_s1 -> conv4/3x3_s1
I0415 15:47:04.216126 32479 net.cpp:113] Setting up conv4/3x3_s1
I0415 15:47:04.221101 32479 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0415 15:47:04.221110 32479 layer_factory.hpp:74] Creating layer conv4/relu
I0415 15:47:04.221114 32479 net.cpp:84] Creating Layer conv4/relu
I0415 15:47:04.221117 32479 net.cpp:380] conv4/relu <- conv4/3x3_s1
I0415 15:47:04.221122 32479 net.cpp:327] conv4/relu -> conv4/3x3_s1 (in-place)
I0415 15:47:04.221125 32479 net.cpp:113] Setting up conv4/relu
I0415 15:47:04.221129 32479 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0415 15:47:04.221132 32479 layer_factory.hpp:74] Creating layer conv5/3x3_s1
I0415 15:47:04.221137 32479 net.cpp:84] Creating Layer conv5/3x3_s1
I0415 15:47:04.221139 32479 net.cpp:380] conv5/3x3_s1 <- conv4/3x3_s1
I0415 15:47:04.221145 32479 net.cpp:338] conv5/3x3_s1 -> conv5/3x3_s1
I0415 15:47:04.221150 32479 net.cpp:113] Setting up conv5/3x3_s1
I0415 15:47:04.224385 32479 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0415 15:47:04.224395 32479 layer_factory.hpp:74] Creating layer conv5/relu
I0415 15:47:04.224401 32479 net.cpp:84] Creating Layer conv5/relu
I0415 15:47:04.224405 32479 net.cpp:380] conv5/relu <- conv5/3x3_s1
I0415 15:47:04.224408 32479 net.cpp:327] conv5/relu -> conv5/3x3_s1 (in-place)
I0415 15:47:04.224412 32479 net.cpp:113] Setting up conv5/relu
I0415 15:47:04.224416 32479 net.cpp:120] Top shape: 128 256 13 13 (5537792)
I0415 15:47:04.224418 32479 layer_factory.hpp:74] Creating layer pool5/3x3_s2
I0415 15:47:04.224423 32479 net.cpp:84] Creating Layer pool5/3x3_s2
I0415 15:47:04.224426 32479 net.cpp:380] pool5/3x3_s2 <- conv5/3x3_s1
I0415 15:47:04.224431 32479 net.cpp:338] pool5/3x3_s2 -> pool5/3x3_s2
I0415 15:47:04.224436 32479 net.cpp:113] Setting up pool5/3x3_s2
I0415 15:47:04.224441 32479 net.cpp:120] Top shape: 128 256 6 6 (1179648)
I0415 15:47:04.224443 32479 layer_factory.hpp:74] Creating layer fc6-conv
I0415 15:47:04.224447 32479 net.cpp:84] Creating Layer fc6-conv
I0415 15:47:04.224450 32479 net.cpp:380] fc6-conv <- pool5/3x3_s2
I0415 15:47:04.224455 32479 net.cpp:338] fc6-conv -> fc6-conv
I0415 15:47:04.224459 32479 net.cpp:113] Setting up fc6-conv
I0415 15:47:04.255204 32479 net.cpp:120] Top shape: 128 4096 1 1 (524288)
I0415 15:47:04.255229 32479 layer_factory.hpp:74] Creating layer fc7-conv
I0415 15:47:04.255237 32479 net.cpp:84] Creating Layer fc7-conv
I0415 15:47:04.255241 32479 net.cpp:380] fc7-conv <- fc6-conv
I0415 15:47:04.255246 32479 net.cpp:338] fc7-conv -> fc7-conv
I0415 15:47:04.255252 32479 net.cpp:113] Setting up fc7-conv
I0415 15:47:04.489187 32479 net.cpp:120] Top shape: 128 4096 1 1 (524288)
I0415 15:47:04.489218 32479 layer_factory.hpp:74] Creating layer fc8-conv
I0415 15:47:04.489228 32479 net.cpp:84] Creating Layer fc8-conv
I0415 15:47:04.489231 32479 net.cpp:380] fc8-conv <- fc7-conv
I0415 15:47:04.489238 32479 net.cpp:338] fc8-conv -> fc8-conv
I0415 15:47:04.489245 32479 net.cpp:113] Setting up fc8-conv
I0415 15:47:04.545464 32479 net.cpp:120] Top shape: 128 1000 1 1 (128000)
I0415 15:47:04.545487 32479 net.cpp:169] fc8-conv does not need backward computation.
I0415 15:47:04.545491 32479 net.cpp:169] fc7-conv does not need backward computation.
I0415 15:47:04.545493 32479 net.cpp:169] fc6-conv does not need backward computation.
I0415 15:47:04.545496 32479 net.cpp:169] pool5/3x3_s2 does not need backward computation.
I0415 15:47:04.545500 32479 net.cpp:169] conv5/relu does not need backward computation.
I0415 15:47:04.545501 32479 net.cpp:169] conv5/3x3_s1 does not need backward computation.
I0415 15:47:04.545505 32479 net.cpp:169] conv4/relu does not need backward computation.
I0415 15:47:04.545507 32479 net.cpp:169] conv4/3x3_s1 does not need backward computation.
I0415 15:47:04.545510 32479 net.cpp:169] conv3/relu does not need backward computation.
I0415 15:47:04.545512 32479 net.cpp:169] conv3/3x3_s1 does not need backward computation.
I0415 15:47:04.545514 32479 net.cpp:169] pool2/3x3_s2 does not need backward computation.
I0415 15:47:04.545526 32479 net.cpp:169] cpnv2/relu does not need backward computation.
I0415 15:47:04.545528 32479 net.cpp:169] conv2/5x5_s1 does not need backward computation.
I0415 15:47:04.545531 32479 net.cpp:169] pool1/3x3_s2 does not need backward computation.
I0415 15:47:04.545534 32479 net.cpp:169] conv1/relu does not need backward computation.
I0415 15:47:04.545536 32479 net.cpp:169] conv1 does not need backward computation.
I0415 15:47:04.545541 32479 net.cpp:205] This network produces output fc8-conv
I0415 15:47:04.545557 32479 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0415 15:47:04.545563 32479 net.cpp:217] Network initialization done.
I0415 15:47:04.545567 32479 net.cpp:218] Memory required for data: 546557952
I0415 15:47:04.545626 32479 caffe.cpp:224] Performing Forward
I0415 15:47:04.988239 32479 caffe.cpp:229] Initial loss: 0
I0415 15:47:04.988276 32479 caffe.cpp:230] Performing Backward
I0415 15:47:05.681794 32479 caffe.cpp:238] *** Benchmark begins ***
I0415 15:47:05.681805 32479 caffe.cpp:239] Testing for 10 iterations.
I0415 15:47:06.812775 32479 caffe.cpp:270] Iteration: 1 forward-backward time: 1095.39 ms.
I0415 15:47:07.908825 32479 caffe.cpp:270] Iteration: 2 forward-backward time: 1096 ms.
I0415 15:47:09.004791 32479 caffe.cpp:270] Iteration: 3 forward-backward time: 1095.92 ms.
I0415 15:47:10.100219 32479 caffe.cpp:270] Iteration: 4 forward-backward time: 1095.38 ms.
I0415 15:47:11.195647 32479 caffe.cpp:270] Iteration: 5 forward-backward time: 1095.38 ms.
I0415 15:47:12.291049 32479 caffe.cpp:270] Iteration: 6 forward-backward time: 1095.35 ms.
I0415 15:47:13.386924 32479 caffe.cpp:270] Iteration: 7 forward-backward time: 1095.82 ms.
I0415 15:47:14.482609 32479 caffe.cpp:270] Iteration: 8 forward-backward time: 1095.63 ms.
I0415 15:47:15.578457 32479 caffe.cpp:270] Iteration: 9 forward-backward time: 1095.8 ms.
I0415 15:47:16.675181 32479 caffe.cpp:270] Iteration: 10 forward-backward time: 1096.67 ms.
I0415 15:47:16.675196 32479 caffe.cpp:273] Average time per layer: 
I0415 15:47:16.675200 32479 caffe.cpp:276]      conv1	forward: 16.0871 ms.
I0415 15:47:16.675204 32479 caffe.cpp:279]      conv1	backward: 42.0226 ms.
I0415 15:47:16.675207 32479 caffe.cpp:276] conv1/relu	forward: 0.785002 ms.
I0415 15:47:16.675211 32479 caffe.cpp:279] conv1/relu	backward: 1.16879 ms.
I0415 15:47:16.675215 32479 caffe.cpp:276] pool1/3x3_s2	forward: 0.881283 ms.
I0415 15:47:16.675217 32479 caffe.cpp:279] pool1/3x3_s2	backward: 3.53495 ms.
I0415 15:47:16.675220 32479 caffe.cpp:276] conv2/5x5_s1	forward: 30.1649 ms.
I0415 15:47:16.675223 32479 caffe.cpp:279] conv2/5x5_s1	backward: 47.2468 ms.
I0415 15:47:16.675226 32479 caffe.cpp:276] cpnv2/relu	forward: 0.568422 ms.
I0415 15:47:16.675230 32479 caffe.cpp:279] cpnv2/relu	backward: 0.846336 ms.
I0415 15:47:16.675232 32479 caffe.cpp:276] pool2/3x3_s2	forward: 0.65393 ms.
I0415 15:47:16.675235 32479 caffe.cpp:279] pool2/3x3_s2	backward: 2.60547 ms.
I0415 15:47:16.675237 32479 caffe.cpp:276] conv3/3x3_s1	forward: 22.556 ms.
I0415 15:47:16.675240 32479 caffe.cpp:279] conv3/3x3_s1	backward: 32.7431 ms.
I0415 15:47:16.675243 32479 caffe.cpp:276] conv3/relu	forward: 0.26584 ms.
I0415 15:47:16.675246 32479 caffe.cpp:279] conv3/relu	backward: 0.393331 ms.
I0415 15:47:16.675250 32479 caffe.cpp:276] conv4/3x3_s1	forward: 28.3665 ms.
I0415 15:47:16.675252 32479 caffe.cpp:279] conv4/3x3_s1	backward: 40.8579 ms.
I0415 15:47:16.675256 32479 caffe.cpp:276] conv4/relu	forward: 0.177568 ms.
I0415 15:47:16.675258 32479 caffe.cpp:279] conv4/relu	backward: 0.265936 ms.
I0415 15:47:16.675261 32479 caffe.cpp:276] conv5/3x3_s1	forward: 20.9309 ms.
I0415 15:47:16.675264 32479 caffe.cpp:279] conv5/3x3_s1	backward: 28.9223 ms.
I0415 15:47:16.675267 32479 caffe.cpp:276] conv5/relu	forward: 0.177354 ms.
I0415 15:47:16.675271 32479 caffe.cpp:279] conv5/relu	backward: 0.266765 ms.
I0415 15:47:16.675272 32479 caffe.cpp:276] pool5/3x3_s2	forward: 0.222211 ms.
I0415 15:47:16.675276 32479 caffe.cpp:279] pool5/3x3_s2	backward: 0.813466 ms.
I0415 15:47:16.675285 32479 caffe.cpp:276]   fc6-conv	forward: 177.797 ms.
I0415 15:47:16.675289 32479 caffe.cpp:279]   fc6-conv	backward: 317.324 ms.
I0415 15:47:16.675292 32479 caffe.cpp:276]   fc7-conv	forward: 73.305 ms.
I0415 15:47:16.675295 32479 caffe.cpp:279]   fc7-conv	backward: 143.787 ms.
I0415 15:47:16.675298 32479 caffe.cpp:276]   fc8-conv	forward: 22.3899 ms.
I0415 15:47:16.675302 32479 caffe.cpp:279]   fc8-conv	backward: 37.3508 ms.
I0415 15:47:16.675313 32479 caffe.cpp:284] Average Forward pass: 395.445 ms.
I0415 15:47:16.675318 32479 caffe.cpp:286] Average Backward pass: 700.274 ms.
I0415 15:47:16.675323 32479 caffe.cpp:288] Average Forward-Backward: 1095.77 ms.
I0415 15:47:16.675326 32479 caffe.cpp:290] Total Time: 10957.7 ms.
I0415 15:47:16.675329 32479 caffe.cpp:291] *** Benchmark ends ***
