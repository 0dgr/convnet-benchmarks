I0408 23:27:55.545164 11315 caffe.cpp:212] Use GPU with device ID 0
E0408 23:27:55.788975 11315 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./imagenet_winners/overfeat.prototxt
I0408 23:27:55.789047 11315 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0408 23:27:55.789196 11315 net.cpp:42] Initializing net from parameters: 
name: "overfeat"
input: "data"
input_dim: 128
input_dim: 3
input_dim: 231
input_dim: 231
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv1/11x11_s4"
  type: "Convolution"
  bottom: "data"
  top: "conv1/11x11_s4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv1/relu"
  type: "ReLU"
  bottom: "conv1/11x11_s4"
  top: "conv1/11x11_s4"
}
layer {
  name: "pool1/2x2_s2"
  type: "Pooling"
  bottom: "conv1/11x11_s4"
  top: "pool1/2x2_s2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2/5x5_s1"
  type: "Convolution"
  bottom: "pool1/2x2_s2"
  top: "conv2/5x5_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv2/relu"
  type: "ReLU"
  bottom: "conv2/5x5_s1"
  top: "conv2/5x5_s1"
}
layer {
  name: "pool2/2x2_s2"
  type: "Pooling"
  bottom: "conv2/5x5_s1"
  top: "pool2/2x2_s2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3/3x3_s1"
  type: "Convolution"
  bottom: "pool2/2x2_s2"
  top: "conv3/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv3/relu"
  type: "ReLU"
  bottom: "conv3/3x3_s1"
  top: "conv3/3x3_s1"
}
layer {
  name: "conv4/3x3_s1"
  type: "Convolution"
  bottom: "conv3/3x3_s1"
  top: "conv4/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1024
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv4/relu"
  type: "ReLU"
  bottom: "conv4/3x3_s1"
  top: "conv4/3x3_s1"
}
layer {
  name: "conv5/3x3_s1"
  type: "Convolution"
  bottom: "conv4/3x3_s1"
  top: "conv5/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1024
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv5/relu"
  type: "ReLU"
  bottom: "conv5/3x3_s1"
  top: "conv5/3x3_s1"
}
layer {
  name: "pool5/2x2_s2"
  type: "Pooling"
  bottom: "conv5/3x3_s1"
  top: "pool5/2x2_s2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc6-conv"
  type: "Convolution"
  bottom: "pool5/2x2_s2"
  top: "fc6-conv"
  convolution_param {
    num_output: 3072
    kernel_size: 6
  }
}
layer {
  name: "fc7-conv"
  type: "Convolution"
  bottom: "fc6-conv"
  top: "fc7-conv"
  convolution_param {
    num_output: 4096
    kernel_size: 1
  }
}
layer {
  name: "fc8-conv"
  type: "Convolution"
  bottom: "fc7-conv"
  top: "fc8-conv"
  convolution_param {
    num_output: 1000
    kernel_size: 1
  }
}
I0408 23:27:55.789253 11315 net.cpp:340] Input 0 -> data
I0408 23:27:55.789296 11315 layer_factory.hpp:74] Creating layer conv1/11x11_s4
I0408 23:27:55.789315 11315 net.cpp:84] Creating Layer conv1/11x11_s4
I0408 23:27:55.789319 11315 net.cpp:380] conv1/11x11_s4 <- data
I0408 23:27:55.789326 11315 net.cpp:338] conv1/11x11_s4 -> conv1/11x11_s4
I0408 23:27:55.789336 11315 net.cpp:113] Setting up conv1/11x11_s4
I0408 23:27:55.789909 11315 net.cpp:120] Top shape: 128 96 56 56 (38535168)
I0408 23:27:55.789924 11315 layer_factory.hpp:74] Creating layer conv1/relu
I0408 23:27:55.789930 11315 net.cpp:84] Creating Layer conv1/relu
I0408 23:27:55.789933 11315 net.cpp:380] conv1/relu <- conv1/11x11_s4
I0408 23:27:55.789938 11315 net.cpp:327] conv1/relu -> conv1/11x11_s4 (in-place)
I0408 23:27:55.789942 11315 net.cpp:113] Setting up conv1/relu
I0408 23:27:55.789952 11315 net.cpp:120] Top shape: 128 96 56 56 (38535168)
I0408 23:27:55.789955 11315 layer_factory.hpp:74] Creating layer pool1/2x2_s2
I0408 23:27:55.789960 11315 net.cpp:84] Creating Layer pool1/2x2_s2
I0408 23:27:55.789963 11315 net.cpp:380] pool1/2x2_s2 <- conv1/11x11_s4
I0408 23:27:55.789968 11315 net.cpp:338] pool1/2x2_s2 -> pool1/2x2_s2
I0408 23:27:55.789973 11315 net.cpp:113] Setting up pool1/2x2_s2
I0408 23:27:55.789985 11315 net.cpp:120] Top shape: 128 96 28 28 (9633792)
I0408 23:27:55.789989 11315 layer_factory.hpp:74] Creating layer conv2/5x5_s1
I0408 23:27:55.789994 11315 net.cpp:84] Creating Layer conv2/5x5_s1
I0408 23:27:55.789998 11315 net.cpp:380] conv2/5x5_s1 <- pool1/2x2_s2
I0408 23:27:55.790001 11315 net.cpp:338] conv2/5x5_s1 -> conv2/5x5_s1
I0408 23:27:55.790009 11315 net.cpp:113] Setting up conv2/5x5_s1
I0408 23:27:55.793822 11315 net.cpp:120] Top shape: 128 256 24 24 (18874368)
I0408 23:27:55.793848 11315 layer_factory.hpp:74] Creating layer conv2/relu
I0408 23:27:55.793856 11315 net.cpp:84] Creating Layer conv2/relu
I0408 23:27:55.793859 11315 net.cpp:380] conv2/relu <- conv2/5x5_s1
I0408 23:27:55.793865 11315 net.cpp:327] conv2/relu -> conv2/5x5_s1 (in-place)
I0408 23:27:55.793871 11315 net.cpp:113] Setting up conv2/relu
I0408 23:27:55.793875 11315 net.cpp:120] Top shape: 128 256 24 24 (18874368)
I0408 23:27:55.793879 11315 layer_factory.hpp:74] Creating layer pool2/2x2_s2
I0408 23:27:55.793884 11315 net.cpp:84] Creating Layer pool2/2x2_s2
I0408 23:27:55.793887 11315 net.cpp:380] pool2/2x2_s2 <- conv2/5x5_s1
I0408 23:27:55.793891 11315 net.cpp:338] pool2/2x2_s2 -> pool2/2x2_s2
I0408 23:27:55.793897 11315 net.cpp:113] Setting up pool2/2x2_s2
I0408 23:27:55.793905 11315 net.cpp:120] Top shape: 128 256 12 12 (4718592)
I0408 23:27:55.793908 11315 layer_factory.hpp:74] Creating layer conv3/3x3_s1
I0408 23:27:55.793915 11315 net.cpp:84] Creating Layer conv3/3x3_s1
I0408 23:27:55.793917 11315 net.cpp:380] conv3/3x3_s1 <- pool2/2x2_s2
I0408 23:27:55.793922 11315 net.cpp:338] conv3/3x3_s1 -> conv3/3x3_s1
I0408 23:27:55.793928 11315 net.cpp:113] Setting up conv3/3x3_s1
I0408 23:27:55.800875 11315 net.cpp:120] Top shape: 128 512 12 12 (9437184)
I0408 23:27:55.800904 11315 layer_factory.hpp:74] Creating layer conv3/relu
I0408 23:27:55.800912 11315 net.cpp:84] Creating Layer conv3/relu
I0408 23:27:55.800916 11315 net.cpp:380] conv3/relu <- conv3/3x3_s1
I0408 23:27:55.800921 11315 net.cpp:327] conv3/relu -> conv3/3x3_s1 (in-place)
I0408 23:27:55.800928 11315 net.cpp:113] Setting up conv3/relu
I0408 23:27:55.800933 11315 net.cpp:120] Top shape: 128 512 12 12 (9437184)
I0408 23:27:55.800936 11315 layer_factory.hpp:74] Creating layer conv4/3x3_s1
I0408 23:27:55.800945 11315 net.cpp:84] Creating Layer conv4/3x3_s1
I0408 23:27:55.800947 11315 net.cpp:380] conv4/3x3_s1 <- conv3/3x3_s1
I0408 23:27:55.800952 11315 net.cpp:338] conv4/3x3_s1 -> conv4/3x3_s1
I0408 23:27:55.800958 11315 net.cpp:113] Setting up conv4/3x3_s1
I0408 23:27:55.827428 11315 net.cpp:120] Top shape: 128 1024 12 12 (18874368)
I0408 23:27:55.827455 11315 layer_factory.hpp:74] Creating layer conv4/relu
I0408 23:27:55.827463 11315 net.cpp:84] Creating Layer conv4/relu
I0408 23:27:55.827467 11315 net.cpp:380] conv4/relu <- conv4/3x3_s1
I0408 23:27:55.827472 11315 net.cpp:327] conv4/relu -> conv4/3x3_s1 (in-place)
I0408 23:27:55.827486 11315 net.cpp:113] Setting up conv4/relu
I0408 23:27:55.827492 11315 net.cpp:120] Top shape: 128 1024 12 12 (18874368)
I0408 23:27:55.827496 11315 layer_factory.hpp:74] Creating layer conv5/3x3_s1
I0408 23:27:55.827503 11315 net.cpp:84] Creating Layer conv5/3x3_s1
I0408 23:27:55.827507 11315 net.cpp:380] conv5/3x3_s1 <- conv4/3x3_s1
I0408 23:27:55.827512 11315 net.cpp:338] conv5/3x3_s1 -> conv5/3x3_s1
I0408 23:27:55.827519 11315 net.cpp:113] Setting up conv5/3x3_s1
I0408 23:27:55.880350 11315 net.cpp:120] Top shape: 128 1024 12 12 (18874368)
I0408 23:27:55.880380 11315 layer_factory.hpp:74] Creating layer conv5/relu
I0408 23:27:55.880390 11315 net.cpp:84] Creating Layer conv5/relu
I0408 23:27:55.880394 11315 net.cpp:380] conv5/relu <- conv5/3x3_s1
I0408 23:27:55.880400 11315 net.cpp:327] conv5/relu -> conv5/3x3_s1 (in-place)
I0408 23:27:55.880406 11315 net.cpp:113] Setting up conv5/relu
I0408 23:27:55.880411 11315 net.cpp:120] Top shape: 128 1024 12 12 (18874368)
I0408 23:27:55.880414 11315 layer_factory.hpp:74] Creating layer pool5/2x2_s2
I0408 23:27:55.880420 11315 net.cpp:84] Creating Layer pool5/2x2_s2
I0408 23:27:55.880422 11315 net.cpp:380] pool5/2x2_s2 <- conv5/3x3_s1
I0408 23:27:55.880429 11315 net.cpp:338] pool5/2x2_s2 -> pool5/2x2_s2
I0408 23:27:55.880435 11315 net.cpp:113] Setting up pool5/2x2_s2
I0408 23:27:55.880444 11315 net.cpp:120] Top shape: 128 1024 6 6 (4718592)
I0408 23:27:55.880446 11315 layer_factory.hpp:74] Creating layer fc6-conv
I0408 23:27:55.880451 11315 net.cpp:84] Creating Layer fc6-conv
I0408 23:27:55.880455 11315 net.cpp:380] fc6-conv <- pool5/2x2_s2
I0408 23:27:55.880460 11315 net.cpp:338] fc6-conv -> fc6-conv
I0408 23:27:55.880465 11315 net.cpp:113] Setting up fc6-conv
I0408 23:27:56.474176 11315 net.cpp:120] Top shape: 128 3072 1 1 (393216)
I0408 23:27:56.474210 11315 layer_factory.hpp:74] Creating layer fc7-conv
I0408 23:27:56.474220 11315 net.cpp:84] Creating Layer fc7-conv
I0408 23:27:56.474223 11315 net.cpp:380] fc7-conv <- fc6-conv
I0408 23:27:56.474231 11315 net.cpp:338] fc7-conv -> fc7-conv
I0408 23:27:56.474238 11315 net.cpp:113] Setting up fc7-conv
I0408 23:27:56.522461 11315 net.cpp:120] Top shape: 128 4096 1 1 (524288)
I0408 23:27:56.522491 11315 layer_factory.hpp:74] Creating layer fc8-conv
I0408 23:27:56.522502 11315 net.cpp:84] Creating Layer fc8-conv
I0408 23:27:56.522506 11315 net.cpp:380] fc8-conv <- fc7-conv
I0408 23:27:56.522513 11315 net.cpp:338] fc8-conv -> fc8-conv
I0408 23:27:56.522522 11315 net.cpp:113] Setting up fc8-conv
I0408 23:27:56.549222 11315 net.cpp:120] Top shape: 128 1000 1 1 (128000)
I0408 23:27:56.549248 11315 net.cpp:169] fc8-conv does not need backward computation.
I0408 23:27:56.549252 11315 net.cpp:169] fc7-conv does not need backward computation.
I0408 23:27:56.549255 11315 net.cpp:169] fc6-conv does not need backward computation.
I0408 23:27:56.549257 11315 net.cpp:169] pool5/2x2_s2 does not need backward computation.
I0408 23:27:56.549260 11315 net.cpp:169] conv5/relu does not need backward computation.
I0408 23:27:56.549263 11315 net.cpp:169] conv5/3x3_s1 does not need backward computation.
I0408 23:27:56.549265 11315 net.cpp:169] conv4/relu does not need backward computation.
I0408 23:27:56.549268 11315 net.cpp:169] conv4/3x3_s1 does not need backward computation.
I0408 23:27:56.549271 11315 net.cpp:169] conv3/relu does not need backward computation.
I0408 23:27:56.549273 11315 net.cpp:169] conv3/3x3_s1 does not need backward computation.
I0408 23:27:56.549276 11315 net.cpp:169] pool2/2x2_s2 does not need backward computation.
I0408 23:27:56.549279 11315 net.cpp:169] conv2/relu does not need backward computation.
I0408 23:27:56.549283 11315 net.cpp:169] conv2/5x5_s1 does not need backward computation.
I0408 23:27:56.549284 11315 net.cpp:169] pool1/2x2_s2 does not need backward computation.
I0408 23:27:56.549288 11315 net.cpp:169] conv1/relu does not need backward computation.
I0408 23:27:56.549289 11315 net.cpp:169] conv1/11x11_s4 does not need backward computation.
I0408 23:27:56.549302 11315 net.cpp:205] This network produces output fc8-conv
I0408 23:27:56.549319 11315 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0408 23:27:56.549326 11315 net.cpp:217] Network initialization done.
I0408 23:27:56.549329 11315 net.cpp:218] Memory required for data: 917229568
I0408 23:27:56.549391 11315 caffe.cpp:224] Performing Forward
I0408 23:27:57.929229 11315 caffe.cpp:229] Initial loss: 0
I0408 23:27:57.929267 11315 caffe.cpp:230] Performing Backward
I0408 23:28:00.145860 11315 caffe.cpp:238] *** Benchmark begins ***
I0408 23:28:00.145889 11315 caffe.cpp:239] Testing for 10 iterations.
I0413 19:14:14.111778 11745 caffe.cpp:212] Use GPU with device ID 0
E0413 19:14:14.311609 11745 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: ./imagenet_winners/overfeat.prototxt
I0413 19:14:14.311681 11745 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0413 19:14:14.311825 11745 net.cpp:42] Initializing net from parameters: 
name: "overfeat"
input: "data"
input_dim: 128
input_dim: 3
input_dim: 231
input_dim: 231
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv1/11x11_s4"
  type: "Convolution"
  bottom: "data"
  top: "conv1/11x11_s4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv1/relu"
  type: "ReLU"
  bottom: "conv1/11x11_s4"
  top: "conv1/11x11_s4"
}
layer {
  name: "pool1/2x2_s2"
  type: "Pooling"
  bottom: "conv1/11x11_s4"
  top: "pool1/2x2_s2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2/5x5_s1"
  type: "Convolution"
  bottom: "pool1/2x2_s2"
  top: "conv2/5x5_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv2/relu"
  type: "ReLU"
  bottom: "conv2/5x5_s1"
  top: "conv2/5x5_s1"
}
layer {
  name: "pool2/2x2_s2"
  type: "Pooling"
  bottom: "conv2/5x5_s1"
  top: "pool2/2x2_s2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3/3x3_s1"
  type: "Convolution"
  bottom: "pool2/2x2_s2"
  top: "conv3/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv3/relu"
  type: "ReLU"
  bottom: "conv3/3x3_s1"
  top: "conv3/3x3_s1"
}
layer {
  name: "conv4/3x3_s1"
  type: "Convolution"
  bottom: "conv3/3x3_s1"
  top: "conv4/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1024
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv4/relu"
  type: "ReLU"
  bottom: "conv4/3x3_s1"
  top: "conv4/3x3_s1"
}
layer {
  name: "conv5/3x3_s1"
  type: "Convolution"
  bottom: "conv4/3x3_s1"
  top: "conv5/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1024
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv5/relu"
  type: "ReLU"
  bottom: "conv5/3x3_s1"
  top: "conv5/3x3_s1"
}
layer {
  name: "pool5/2x2_s2"
  type: "Pooling"
  bottom: "conv5/3x3_s1"
  top: "pool5/2x2_s2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc6-conv"
  type: "Convolution"
  bottom: "pool5/2x2_s2"
  top: "fc6-conv"
  convolution_param {
    num_output: 3072
    kernel_size: 6
  }
}
layer {
  name: "fc7-conv"
  type: "Convolution"
  bottom: "fc6-conv"
  top: "fc7-conv"
  convolution_param {
    num_output: 4096
    kernel_size: 1
  }
}
layer {
  name: "fc8-conv"
  type: "Convolution"
  bottom: "fc7-conv"
  top: "fc8-conv"
  convolution_param {
    num_output: 1000
    kernel_size: 1
  }
}
I0413 19:14:14.311890 11745 net.cpp:340] Input 0 -> data
I0413 19:14:14.311918 11745 layer_factory.hpp:74] Creating layer conv1/11x11_s4
I0413 19:14:14.311934 11745 net.cpp:84] Creating Layer conv1/11x11_s4
I0413 19:14:14.311939 11745 net.cpp:380] conv1/11x11_s4 <- data
I0413 19:14:14.311945 11745 net.cpp:338] conv1/11x11_s4 -> conv1/11x11_s4
I0413 19:14:14.311954 11745 net.cpp:113] Setting up conv1/11x11_s4
I0413 19:14:14.312520 11745 net.cpp:120] Top shape: 128 96 56 56 (38535168)
I0413 19:14:14.312535 11745 layer_factory.hpp:74] Creating layer conv1/relu
I0413 19:14:14.312541 11745 net.cpp:84] Creating Layer conv1/relu
I0413 19:14:14.312544 11745 net.cpp:380] conv1/relu <- conv1/11x11_s4
I0413 19:14:14.312548 11745 net.cpp:327] conv1/relu -> conv1/11x11_s4 (in-place)
I0413 19:14:14.312553 11745 net.cpp:113] Setting up conv1/relu
I0413 19:14:14.312562 11745 net.cpp:120] Top shape: 128 96 56 56 (38535168)
I0413 19:14:14.312566 11745 layer_factory.hpp:74] Creating layer pool1/2x2_s2
I0413 19:14:14.312571 11745 net.cpp:84] Creating Layer pool1/2x2_s2
I0413 19:14:14.312573 11745 net.cpp:380] pool1/2x2_s2 <- conv1/11x11_s4
I0413 19:14:14.312577 11745 net.cpp:338] pool1/2x2_s2 -> pool1/2x2_s2
I0413 19:14:14.312582 11745 net.cpp:113] Setting up pool1/2x2_s2
I0413 19:14:14.312593 11745 net.cpp:120] Top shape: 128 96 28 28 (9633792)
I0413 19:14:14.312597 11745 layer_factory.hpp:74] Creating layer conv2/5x5_s1
I0413 19:14:14.312602 11745 net.cpp:84] Creating Layer conv2/5x5_s1
I0413 19:14:14.312604 11745 net.cpp:380] conv2/5x5_s1 <- pool1/2x2_s2
I0413 19:14:14.312608 11745 net.cpp:338] conv2/5x5_s1 -> conv2/5x5_s1
I0413 19:14:14.312614 11745 net.cpp:113] Setting up conv2/5x5_s1
I0413 19:14:14.316349 11745 net.cpp:120] Top shape: 128 256 24 24 (18874368)
I0413 19:14:14.316359 11745 layer_factory.hpp:74] Creating layer conv2/relu
I0413 19:14:14.316365 11745 net.cpp:84] Creating Layer conv2/relu
I0413 19:14:14.316368 11745 net.cpp:380] conv2/relu <- conv2/5x5_s1
I0413 19:14:14.316372 11745 net.cpp:327] conv2/relu -> conv2/5x5_s1 (in-place)
I0413 19:14:14.316376 11745 net.cpp:113] Setting up conv2/relu
I0413 19:14:14.316380 11745 net.cpp:120] Top shape: 128 256 24 24 (18874368)
I0413 19:14:14.316383 11745 layer_factory.hpp:74] Creating layer pool2/2x2_s2
I0413 19:14:14.316387 11745 net.cpp:84] Creating Layer pool2/2x2_s2
I0413 19:14:14.316390 11745 net.cpp:380] pool2/2x2_s2 <- conv2/5x5_s1
I0413 19:14:14.316395 11745 net.cpp:338] pool2/2x2_s2 -> pool2/2x2_s2
I0413 19:14:14.316398 11745 net.cpp:113] Setting up pool2/2x2_s2
I0413 19:14:14.316404 11745 net.cpp:120] Top shape: 128 256 12 12 (4718592)
I0413 19:14:14.316407 11745 layer_factory.hpp:74] Creating layer conv3/3x3_s1
I0413 19:14:14.316412 11745 net.cpp:84] Creating Layer conv3/3x3_s1
I0413 19:14:14.316416 11745 net.cpp:380] conv3/3x3_s1 <- pool2/2x2_s2
I0413 19:14:14.316419 11745 net.cpp:338] conv3/3x3_s1 -> conv3/3x3_s1
I0413 19:14:14.316424 11745 net.cpp:113] Setting up conv3/3x3_s1
I0413 19:14:14.322921 11745 net.cpp:120] Top shape: 128 512 12 12 (9437184)
I0413 19:14:14.322932 11745 layer_factory.hpp:74] Creating layer conv3/relu
I0413 19:14:14.322937 11745 net.cpp:84] Creating Layer conv3/relu
I0413 19:14:14.322939 11745 net.cpp:380] conv3/relu <- conv3/3x3_s1
I0413 19:14:14.322943 11745 net.cpp:327] conv3/relu -> conv3/3x3_s1 (in-place)
I0413 19:14:14.322947 11745 net.cpp:113] Setting up conv3/relu
I0413 19:14:14.322952 11745 net.cpp:120] Top shape: 128 512 12 12 (9437184)
I0413 19:14:14.322954 11745 layer_factory.hpp:74] Creating layer conv4/3x3_s1
I0413 19:14:14.322964 11745 net.cpp:84] Creating Layer conv4/3x3_s1
I0413 19:14:14.322968 11745 net.cpp:380] conv4/3x3_s1 <- conv3/3x3_s1
I0413 19:14:14.322973 11745 net.cpp:338] conv4/3x3_s1 -> conv4/3x3_s1
I0413 19:14:14.322978 11745 net.cpp:113] Setting up conv4/3x3_s1
I0413 19:14:14.348822 11745 net.cpp:120] Top shape: 128 1024 12 12 (18874368)
I0413 19:14:14.348846 11745 layer_factory.hpp:74] Creating layer conv4/relu
I0413 19:14:14.348855 11745 net.cpp:84] Creating Layer conv4/relu
I0413 19:14:14.348858 11745 net.cpp:380] conv4/relu <- conv4/3x3_s1
I0413 19:14:14.348863 11745 net.cpp:327] conv4/relu -> conv4/3x3_s1 (in-place)
I0413 19:14:14.348870 11745 net.cpp:113] Setting up conv4/relu
I0413 19:14:14.348875 11745 net.cpp:120] Top shape: 128 1024 12 12 (18874368)
I0413 19:14:14.348877 11745 layer_factory.hpp:74] Creating layer conv5/3x3_s1
I0413 19:14:14.348883 11745 net.cpp:84] Creating Layer conv5/3x3_s1
I0413 19:14:14.348886 11745 net.cpp:380] conv5/3x3_s1 <- conv4/3x3_s1
I0413 19:14:14.348892 11745 net.cpp:338] conv5/3x3_s1 -> conv5/3x3_s1
I0413 19:14:14.348898 11745 net.cpp:113] Setting up conv5/3x3_s1
I0413 19:14:14.400068 11745 net.cpp:120] Top shape: 128 1024 12 12 (18874368)
I0413 19:14:14.400097 11745 layer_factory.hpp:74] Creating layer conv5/relu
I0413 19:14:14.400105 11745 net.cpp:84] Creating Layer conv5/relu
I0413 19:14:14.400110 11745 net.cpp:380] conv5/relu <- conv5/3x3_s1
I0413 19:14:14.400115 11745 net.cpp:327] conv5/relu -> conv5/3x3_s1 (in-place)
I0413 19:14:14.400120 11745 net.cpp:113] Setting up conv5/relu
I0413 19:14:14.400125 11745 net.cpp:120] Top shape: 128 1024 12 12 (18874368)
I0413 19:14:14.400127 11745 layer_factory.hpp:74] Creating layer pool5/2x2_s2
I0413 19:14:14.400135 11745 net.cpp:84] Creating Layer pool5/2x2_s2
I0413 19:14:14.400137 11745 net.cpp:380] pool5/2x2_s2 <- conv5/3x3_s1
I0413 19:14:14.400141 11745 net.cpp:338] pool5/2x2_s2 -> pool5/2x2_s2
I0413 19:14:14.400148 11745 net.cpp:113] Setting up pool5/2x2_s2
I0413 19:14:14.400154 11745 net.cpp:120] Top shape: 128 1024 6 6 (4718592)
I0413 19:14:14.400158 11745 layer_factory.hpp:74] Creating layer fc6-conv
I0413 19:14:14.400164 11745 net.cpp:84] Creating Layer fc6-conv
I0413 19:14:14.400167 11745 net.cpp:380] fc6-conv <- pool5/2x2_s2
I0413 19:14:14.400171 11745 net.cpp:338] fc6-conv -> fc6-conv
I0413 19:14:14.400177 11745 net.cpp:113] Setting up fc6-conv
I0413 19:14:14.832981 11745 net.cpp:120] Top shape: 128 3072 1 1 (393216)
I0413 19:14:14.833012 11745 layer_factory.hpp:74] Creating layer fc7-conv
I0413 19:14:14.833024 11745 net.cpp:84] Creating Layer fc7-conv
I0413 19:14:14.833029 11745 net.cpp:380] fc7-conv <- fc6-conv
I0413 19:14:14.833034 11745 net.cpp:338] fc7-conv -> fc7-conv
I0413 19:14:14.833042 11745 net.cpp:113] Setting up fc7-conv
I0413 19:14:14.942210 11745 net.cpp:120] Top shape: 128 4096 1 1 (524288)
I0413 19:14:14.942237 11745 layer_factory.hpp:74] Creating layer fc8-conv
I0413 19:14:14.942247 11745 net.cpp:84] Creating Layer fc8-conv
I0413 19:14:14.942251 11745 net.cpp:380] fc8-conv <- fc7-conv
I0413 19:14:14.942258 11745 net.cpp:338] fc8-conv -> fc8-conv
I0413 19:14:14.942266 11745 net.cpp:113] Setting up fc8-conv
I0413 19:14:14.977365 11745 net.cpp:120] Top shape: 128 1000 1 1 (128000)
I0413 19:14:14.977392 11745 net.cpp:169] fc8-conv does not need backward computation.
I0413 19:14:14.977396 11745 net.cpp:169] fc7-conv does not need backward computation.
I0413 19:14:14.977399 11745 net.cpp:169] fc6-conv does not need backward computation.
I0413 19:14:14.977401 11745 net.cpp:169] pool5/2x2_s2 does not need backward computation.
I0413 19:14:14.977404 11745 net.cpp:169] conv5/relu does not need backward computation.
I0413 19:14:14.977406 11745 net.cpp:169] conv5/3x3_s1 does not need backward computation.
I0413 19:14:14.977409 11745 net.cpp:169] conv4/relu does not need backward computation.
I0413 19:14:14.977412 11745 net.cpp:169] conv4/3x3_s1 does not need backward computation.
I0413 19:14:14.977414 11745 net.cpp:169] conv3/relu does not need backward computation.
I0413 19:14:14.977424 11745 net.cpp:169] conv3/3x3_s1 does not need backward computation.
I0413 19:14:14.977427 11745 net.cpp:169] pool2/2x2_s2 does not need backward computation.
I0413 19:14:14.977430 11745 net.cpp:169] conv2/relu does not need backward computation.
I0413 19:14:14.977433 11745 net.cpp:169] conv2/5x5_s1 does not need backward computation.
I0413 19:14:14.977435 11745 net.cpp:169] pool1/2x2_s2 does not need backward computation.
I0413 19:14:14.977438 11745 net.cpp:169] conv1/relu does not need backward computation.
I0413 19:14:14.977442 11745 net.cpp:169] conv1/11x11_s4 does not need backward computation.
I0413 19:14:14.977445 11745 net.cpp:205] This network produces output fc8-conv
I0413 19:14:14.977459 11745 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0413 19:14:14.977466 11745 net.cpp:217] Network initialization done.
I0413 19:14:14.977469 11745 net.cpp:218] Memory required for data: 917229568
I0413 19:14:14.977532 11745 caffe.cpp:224] Performing Forward
I0413 19:14:16.009615 11745 caffe.cpp:229] Initial loss: 0
I0413 19:14:16.009651 11745 caffe.cpp:230] Performing Backward
I0413 19:14:17.538489 11745 caffe.cpp:238] *** Benchmark begins ***
I0413 19:14:17.538501 11745 caffe.cpp:239] Testing for 10 iterations.
I0413 19:14:20.122818 11745 caffe.cpp:270] Iteration: 1 forward-backward time: 2535.64 ms.
I0413 19:14:22.657835 11745 caffe.cpp:270] Iteration: 2 forward-backward time: 2534.92 ms.
I0413 19:14:25.195111 11745 caffe.cpp:270] Iteration: 3 forward-backward time: 2537.18 ms.
I0413 19:14:27.732816 11745 caffe.cpp:270] Iteration: 4 forward-backward time: 2537.6 ms.
I0413 19:14:30.269167 11745 caffe.cpp:270] Iteration: 5 forward-backward time: 2536.25 ms.
I0413 19:14:32.806706 11745 caffe.cpp:270] Iteration: 6 forward-backward time: 2537.45 ms.
I0413 19:14:35.343734 11745 caffe.cpp:270] Iteration: 7 forward-backward time: 2536.93 ms.
I0413 19:14:37.881649 11745 caffe.cpp:270] Iteration: 8 forward-backward time: 2537.82 ms.
I0413 19:14:40.442580 11745 caffe.cpp:270] Iteration: 9 forward-backward time: 2560.83 ms.
I0413 19:14:43.006103 11745 caffe.cpp:270] Iteration: 10 forward-backward time: 2563.42 ms.
I0413 19:14:43.006130 11745 caffe.cpp:273] Average time per layer: 
I0413 19:14:43.006134 11745 caffe.cpp:276] conv1/11x11_s4	forward: 27.9592 ms.
I0413 19:14:43.006137 11745 caffe.cpp:279] conv1/11x11_s4	backward: 55.9792 ms.
I0413 19:14:43.006141 11745 caffe.cpp:276] conv1/relu	forward: 1.22298 ms.
I0413 19:14:43.006145 11745 caffe.cpp:279] conv1/relu	backward: 1.81085 ms.
I0413 19:14:43.006147 11745 caffe.cpp:276] pool1/2x2_s2	forward: 0.941171 ms.
I0413 19:14:43.006150 11745 caffe.cpp:279] pool1/2x2_s2	backward: 4.75976 ms.
I0413 19:14:43.006153 11745 caffe.cpp:276] conv2/5x5_s1	forward: 41.5941 ms.
I0413 19:14:43.006156 11745 caffe.cpp:279] conv2/5x5_s1	backward: 65.5255 ms.
I0413 19:14:43.006160 11745 caffe.cpp:276] conv2/relu	forward: 0.59935 ms.
I0413 19:14:43.006163 11745 caffe.cpp:279] conv2/relu	backward: 0.890675 ms.
I0413 19:14:43.006166 11745 caffe.cpp:276] pool2/2x2_s2	forward: 0.470634 ms.
I0413 19:14:43.006168 11745 caffe.cpp:279] pool2/2x2_s2	backward: 2.33687 ms.
I0413 19:14:43.006171 11745 caffe.cpp:276] conv3/3x3_s1	forward: 31.3709 ms.
I0413 19:14:43.006175 11745 caffe.cpp:279] conv3/3x3_s1	backward: 31.8651 ms.
I0413 19:14:43.006178 11745 caffe.cpp:276] conv3/relu	forward: 0.300643 ms.
I0413 19:14:43.006181 11745 caffe.cpp:279] conv3/relu	backward: 0.446691 ms.
I0413 19:14:43.006183 11745 caffe.cpp:276] conv4/3x3_s1	forward: 84.0957 ms.
I0413 19:14:43.006187 11745 caffe.cpp:279] conv4/3x3_s1	backward: 101.85 ms.
I0413 19:14:43.006191 11745 caffe.cpp:276] conv4/relu	forward: 0.599373 ms.
I0413 19:14:43.006193 11745 caffe.cpp:279] conv4/relu	backward: 0.889562 ms.
I0413 19:14:43.006196 11745 caffe.cpp:276] conv5/3x3_s1	forward: 165.967 ms.
I0413 19:14:43.006199 11745 caffe.cpp:279] conv5/3x3_s1	backward: 195.194 ms.
I0413 19:14:43.006202 11745 caffe.cpp:276] conv5/relu	forward: 0.601315 ms.
I0413 19:14:43.006206 11745 caffe.cpp:279] conv5/relu	backward: 0.891808 ms.
I0413 19:14:43.006217 11745 caffe.cpp:276] pool5/2x2_s2	forward: 0.48816 ms.
I0413 19:14:43.006219 11745 caffe.cpp:279] pool5/2x2_s2	backward: 2.35161 ms.
I0413 19:14:43.006222 11745 caffe.cpp:276]   fc6-conv	forward: 552.866 ms.
I0413 19:14:43.006227 11745 caffe.cpp:279]   fc6-conv	backward: 942.052 ms.
I0413 19:14:43.006229 11745 caffe.cpp:276]   fc7-conv	forward: 60.4247 ms.
I0413 19:14:43.006232 11745 caffe.cpp:279]   fc7-conv	backward: 105.219 ms.
I0413 19:14:43.006235 11745 caffe.cpp:276]   fc8-conv	forward: 22.5562 ms.
I0413 19:14:43.006238 11745 caffe.cpp:279]   fc8-conv	backward: 37.3866 ms.
I0413 19:14:43.006250 11745 caffe.cpp:284] Average Forward pass: 992.195 ms.
I0413 19:14:43.006254 11745 caffe.cpp:286] Average Backward pass: 1549.59 ms.
I0413 19:14:43.006259 11745 caffe.cpp:288] Average Forward-Backward: 2541.85 ms.
I0413 19:14:43.006263 11745 caffe.cpp:290] Total Time: 25418.6 ms.
I0413 19:14:43.006268 11745 caffe.cpp:291] *** Benchmark ends ***
